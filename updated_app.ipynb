{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\amzn ml challenge\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load constants\n",
    "entity_unit_map = {\n",
    "    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'item_weight': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "    'maximum_weight_recommendation': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "    'voltage': {'kilovolt', 'millivolt', 'volt'},\n",
    "    'wattage': {'kilowatt', 'watt'},\n",
    "    'item_volume': {'centilitre', 'cubic foot', 'cubic inch', 'cup', 'decilitre', 'fluid ounce', 'gallon', \n",
    "                    'imperial gallon', 'litre', 'microlitre', 'millilitre', 'pint', 'quart'}\n",
    "}\n",
    "\n",
    "allowed_units = {unit for entity in entity_unit_map for unit in entity_unit_map[entity]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to download images from URLs\n",
    "def download_image(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return Image.open(BytesIO(response.content))\n",
    "    return None\n",
    "\n",
    "# Function to apply OCR and extract text\n",
    "def extract_text_from_image(image):\n",
    "    return pytesseract.image_to_string(image)\n",
    "\n",
    "# Function to clean and extract value + unit\n",
    "def extract_value_and_unit(ocr_text, entity_name):\n",
    "    entity_units = entity_unit_map.get(entity_name, allowed_units)\n",
    "    # Search for numbers + units in OCR text\n",
    "    for unit in entity_units:\n",
    "        if unit in ocr_text:\n",
    "            # Extract the value before the unit\n",
    "            try:\n",
    "                value = float(ocr_text.split(unit)[0].strip())\n",
    "                return f\"{value} {unit}\"\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return \"\"\n",
    "\n",
    "# Hugging Face model pipeline for image captioning (optional)\n",
    "def image_captioning(image):\n",
    "    caption_pipeline = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "    caption = caption_pipeline(image)[0]['generated_text']\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\amzn ml challenge\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize GPT-2 tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# GPT-2 doesn't have a pad_token by default, so we add one\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use the eos_token as the pad_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation function\n",
    "def generate_entity_extraction(text, entity_name):\n",
    "    # Prepare the input with attention mask\n",
    "    inputs = tokenizer(\n",
    "        f\"Extract the {entity_name} from the following text:\\n\\n{text}\\n\\n\",\n",
    "        return_tensors=\"pt\",  # Return as PyTorch tensors\n",
    "        padding=True,         # Explicitly enable padding\n",
    "        truncation=True,      # Truncate if the text is too long\n",
    "        add_special_tokens=True  # Add special tokens if necessary\n",
    "    )\n",
    "\n",
    "    # Add attention mask to avoid warning\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Generate response\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,  # Pass attention mask\n",
    "        max_new_tokens=50,              # Set max tokens to control length\n",
    "        do_sample=True,                 # Sampling for diverse outputs\n",
    "        temperature=0.7                 # Adjust temperature for creativity\n",
    "    )\n",
    "\n",
    "    # Decode the generated tokens\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main prediction function\n",
    "def generate_predictions(test_file, output_file):\n",
    "    # Read the test data\n",
    "    with open(test_file, 'r') as test_f, open(output_file, 'w') as out_f:\n",
    "        test_reader = csv.DictReader(test_f)\n",
    "        fieldnames = ['index', 'prediction']\n",
    "        writer = csv.DictWriter(out_f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in test_reader:\n",
    "            index = row['index']\n",
    "            image_url = row['image_link']\n",
    "            entity_name = row['entity_name']\n",
    "            \n",
    "            # Download image\n",
    "            image = download_image(image_url)\n",
    "            if image is None:\n",
    "                writer.writerow({'index': index, 'prediction': ''})\n",
    "                continue\n",
    "\n",
    "            # Apply OCR to extract text from image\n",
    "            ocr_text = extract_text_from_image(image)\n",
    "\n",
    "            # Optional: Generate image caption (if needed)\n",
    "            caption = image_captioning(image)\n",
    "            ocr_text += \" \" + caption  # Append caption to OCR results\n",
    "\n",
    "            # Use OCR to extract value + unit\n",
    "            prediction = extract_value_and_unit(ocr_text, entity_name)\n",
    "            \n",
    "            # If no prediction from OCR, attempt using local GPT-Neo model\n",
    "            if not prediction:\n",
    "                prediction = generate_entity_extraction(ocr_text, entity_name)\n",
    "\n",
    "            writer.writerow({'index': index, 'prediction': prediction})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "d:\\amzn ml challenge\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "test_file_path = 'dataset/sample_test.csv'\n",
    "output_file_path = 'sample_output.csv'\n",
    "generate_predictions(test_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\amzn ml challenge\\venv\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.7.1) was trained with spaCy v3.7.2 and may not be 100% compatible with the current version (3.8.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.cli import download\n",
    "\n",
    "# Attempt to load the spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    # If loading fails, download and install the model\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Now you can use `nlp` as usual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define allowed units\n",
    "allowed_units = {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard',\n",
    "                  'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton',\n",
    "                  'kilovolt', 'millivolt', 'volt', 'kilowatt', 'watt',\n",
    "                  'centilitre', 'cubic foot', 'cubic inch', 'cup', 'decilitre', 'fluid ounce',\n",
    "                  'gallon', 'imperial gallon', 'litre', 'microlitre', 'millilitre', 'pint', 'quart'}\n",
    "\n",
    "def extract_units(text, entity_type):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    units = set()\n",
    "    for token in doc:\n",
    "        if token.text.lower() in allowed_units:\n",
    "            units.add(token.text.lower())\n",
    "    return units\n",
    "\n",
    "def process_csv(input_file, output_file):\n",
    "    # Read CSV\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process each row\n",
    "    for index, row in df.iterrows():\n",
    "        prediction = row['prediction']\n",
    "        if pd.isna(prediction) or not prediction:\n",
    "            results.append({'index': index, 'prediction': ''})\n",
    "            continue\n",
    "        \n",
    "        # Example entity types (can be expanded as needed)\n",
    "        if 'width' in prediction.lower():\n",
    "            entity_type = 'width'\n",
    "        elif 'height' in prediction.lower():\n",
    "            entity_type = 'height'\n",
    "        elif 'depth' in prediction.lower():\n",
    "            entity_type = 'depth'\n",
    "        elif 'item_weight' in prediction.lower():\n",
    "            entity_type = 'item_weight'\n",
    "        elif 'voltage' in prediction.lower():\n",
    "            entity_type = 'voltage'\n",
    "        elif 'wattage' in prediction.lower():\n",
    "            entity_type = 'wattage'\n",
    "        elif 'item_volume' in prediction.lower():\n",
    "            entity_type = 'item_volume'\n",
    "        else:\n",
    "            entity_type = None\n",
    "\n",
    "        if entity_type:\n",
    "            units = extract_units(prediction, entity_type)\n",
    "            result = f\"{entity_type}: \" + \", \".join(units) if units else \"\"\n",
    "            results.append({'index': index, 'prediction': result})\n",
    "        else:\n",
    "            results.append({'index': index, 'prediction': ''})\n",
    "\n",
    "    # Save results to CSV\n",
    "    result_df = pd.DataFrame(results)\n",
    "    result_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output file paths\n",
    "input_file = 'sample_output.csv'\n",
    "output_file = 'processed_output.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the CSV file\n",
    "process_csv(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
