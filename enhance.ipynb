{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for loading image and text data\n",
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.data_frame['encoded_units'] = self.label_encoder.fit_transform(self.data_frame['entity_value'].apply(self.extract_unit))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_url = self.data_frame.iloc[idx, 0]\n",
    "        entity_value = self.data_frame.iloc[idx, 3]\n",
    "        image = Image.open(requests.get(img_url, stream=True).raw)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Use regex to extract the numeric part of entity_value\n",
    "        match = re.search(r'\\d+(\\.\\d+)?', entity_value)  # Match float or integer numbers\n",
    "        if match:\n",
    "            value = float(match.group())  # Extract the numeric part as a float\n",
    "        else:\n",
    "            raise ValueError(f\"No numeric value found in entity_value: {entity_value}\")\n",
    "\n",
    "        unit = self.extract_unit(entity_value)\n",
    "        encoded_unit = self.label_encoder.transform([unit])[0]\n",
    "\n",
    "        # Return image, numeric value, and encoded unit\n",
    "        return image, torch.tensor(value, dtype=torch.float32), torch.tensor(encoded_unit, dtype=torch.long)\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_unit(entity_value):\n",
    "        # Extract unit from entity_value (improve as needed)\n",
    "        match = re.search(r'([a-zA-Z]+)', entity_value)\n",
    "        return match.group() if match else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_dataset = ImageTextDataset(csv_file='training_data.csv', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\amzn ml challenge\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\amzn ml challenge\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Create a simple ResNet model with separate branches for value and unit prediction\n",
    "class ResNetWithUnits(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetWithUnits, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.fc = torch.nn.Linear(self.resnet.fc.in_features, 512)  # Intermediate layer\n",
    "        \n",
    "        self.value_head = torch.nn.Linear(512, 1)\n",
    "        self.unit_head = torch.nn.Linear(512, len(train_dataset.label_encoder.classes_))\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.resnet(x)\n",
    "        value = self.value_head(features)\n",
    "        unit = self.unit_head(features)\n",
    "        return value, unit\n",
    "\n",
    "model = ResNetWithUnits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model (simplified)\n",
    "criterion_value = torch.nn.MSELoss()  # Mean Squared Error for regression\n",
    "criterion_unit = torch.nn.CrossEntropyLoss()  # Cross-Entropy Loss for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 36930.13671875\n",
      "Epoch 2, Loss: 7418.09912109375\n",
      "Epoch 3, Loss: 640889.75\n",
      "Epoch 4, Loss: 7154.5400390625\n",
      "Epoch 5, Loss: 120255.4765625\n",
      "Epoch 6, Loss: 67426.6484375\n",
      "Epoch 7, Loss: 75930.3359375\n",
      "Epoch 8, Loss: 32710.63671875\n",
      "Epoch 9, Loss: 61815.08984375\n",
      "Epoch 10, Loss: 14663.91015625\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(10):  # Example: Train for 10 epochs\n",
    "    for images, labels_value, labels_unit in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs_value, outputs_unit = model(images)\n",
    "        loss_value = criterion_value(outputs_value.squeeze(), labels_value)\n",
    "        loss_unit = criterion_unit(outputs_unit, labels_unit)\n",
    "        loss = loss_value + loss_unit\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'trained_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value: 111.89685821533203, Unit: gram\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set model to evaluation mode\n",
    "image = Image.open('images/10.jpg')\n",
    "image = transform(image).unsqueeze(0)  # Apply transformations and add batch dimension\n",
    "with torch.no_grad():\n",
    "    predicted_value, predicted_unit = model(image)\n",
    "    unit = train_dataset.label_encoder.inverse_transform([predicted_unit.argmax(dim=1).item()])[0]\n",
    "    print(f'Predicted Value: {predicted_value.item()}, Unit: {unit}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
