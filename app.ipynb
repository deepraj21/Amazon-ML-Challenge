{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load constants\n",
    "entity_unit_map = {\n",
    "    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'item_weight': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "    'maximum_weight_recommendation': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "    'voltage': {'kilovolt', 'millivolt', 'volt'},\n",
    "    'wattage': {'kilowatt', 'watt'},\n",
    "    'item_volume': {'centilitre', 'cubic foot', 'cubic inch', 'cup', 'decilitre', 'fluid ounce', 'gallon', \n",
    "                    'imperial gallon', 'litre', 'microlitre', 'millilitre', 'pint', 'quart'}\n",
    "}\n",
    "\n",
    "allowed_units = {unit for entity in entity_unit_map for unit in entity_unit_map[entity]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download images from URLs\n",
    "def download_image(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return Image.open(BytesIO(response.content))\n",
    "    return None\n",
    "\n",
    "# Function to apply OCR and extract text\n",
    "def extract_text_from_image(image):\n",
    "    return pytesseract.image_to_string(image)\n",
    "\n",
    "# Function to clean and extract value + unit\n",
    "def extract_value_and_unit(ocr_text, entity_name):\n",
    "    entity_units = entity_unit_map.get(entity_name, allowed_units)\n",
    "    print(f\"OCR Text: {ocr_text}\")  # Debugging print for OCR output\n",
    "    \n",
    "    # Regular expression to find numbers followed by units\n",
    "    for unit in entity_units:\n",
    "        pattern = rf\"(\\d+\\.?\\d*)\\s*{unit}\"\n",
    "        match = re.search(pattern, ocr_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            value = match.group(1)\n",
    "            print(f\"Match found: {value} {unit}\")  # Debugging print for matched value and unit\n",
    "            return f\"{value} {unit}\"\n",
    "    \n",
    "    print(f\"No match found for entity {entity_name}\")  # Debugging print if no match\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# Fallback function to extract probable numbers if no entity name is found\n",
    "def extract_probable_numbers(ocr_text):\n",
    "    # Regular expression to capture any number, including floating-point\n",
    "    pattern = r\"(\\d+\\.?\\d*)\"\n",
    "    matches = re.findall(pattern, ocr_text)\n",
    "    \n",
    "    if matches:\n",
    "        probable_numbers = \", \".join(matches)  # Join all numbers found\n",
    "        print(f\"Probable numbers found: {probable_numbers}\")  # Debugging print for probable numbers\n",
    "        return probable_numbers\n",
    "    else:\n",
    "        print(\"No numbers found\")  # Debugging print if no numbers found\n",
    "        return \"\"\n",
    "\n",
    "# Hugging Face model pipeline for image captioning (optional)\n",
    "def image_captioning(image):\n",
    "    caption_pipeline = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "    caption = caption_pipeline(image)[0]['generated_text']\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GPT-2 tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# GPT-2 doesn't have a pad_token by default, so we add one\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use the eos_token as the pad_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_entity_extraction(text, entity_name):\n",
    "    # Prepare the input with attention mask\n",
    "    inputs = tokenizer(\n",
    "        f\"Extract the {entity_name} from the following text:\\n\\n{text}\\n\\n\",\n",
    "        return_tensors=\"pt\",  # Return as PyTorch tensors\n",
    "        padding=True,         # Explicitly enable padding\n",
    "        truncation=True,      # Truncate if the text is too long\n",
    "        add_special_tokens=True  # Add special tokens if necessary\n",
    "    )\n",
    "\n",
    "    # Add attention mask to avoid warning\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Generate response\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,  # Pass attention mask\n",
    "        max_new_tokens=50,              # Set max tokens to control length\n",
    "        do_sample=True,                 # Sampling for diverse outputs\n",
    "        temperature=0.7                 # Adjust temperature for creativity\n",
    "    )\n",
    "\n",
    "    # Decode the generated tokens\n",
    "    gpt_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"GPT Output: {gpt_output}\")  # Debugging print for GPT output\n",
    "\n",
    "    # Post-process GPT output to extract numeric values and allowed units\n",
    "    entity_units = entity_unit_map.get(entity_name, allowed_units)\n",
    "    for unit in entity_units:\n",
    "        pattern = rf\"(\\d+\\.?\\d*)\\s*{unit}\"\n",
    "        match = re.search(pattern, gpt_output, re.IGNORECASE)\n",
    "        if match:\n",
    "            value = match.group(1)\n",
    "            print(f\"Match found in GPT: {value} {unit}\")  # Debugging print for matched value and unit from GPT\n",
    "            return f\"{value} {unit}\"\n",
    "    \n",
    "    print(f\"No match found in GPT for entity {entity_name}\")  # Debugging print if no match in GPT output\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main prediction function\n",
    "def generate_predictions(test_file, output_file):\n",
    "    # Read the test data\n",
    "    with open(test_file, 'r') as test_f, open(output_file, 'w') as out_f:\n",
    "        test_reader = csv.DictReader(test_f)\n",
    "        fieldnames = ['index', 'prediction', 'fallback_numbers']\n",
    "        writer = csv.DictWriter(out_f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in test_reader:\n",
    "            index = row['index']\n",
    "            image_url = row['image_link']\n",
    "            entity_name = row['entity_name']\n",
    "            \n",
    "            # Download image\n",
    "            image = download_image(image_url)\n",
    "            if image is None:\n",
    "                writer.writerow({'index': index, 'prediction': '', 'fallback_numbers': ''})\n",
    "                continue\n",
    "\n",
    "            # Apply OCR to extract text from image\n",
    "            ocr_text = extract_text_from_image(image)\n",
    "\n",
    "            # Optional: Generate image caption (if needed)\n",
    "            caption = image_captioning(image)\n",
    "            ocr_text += \" \" + caption  # Append caption to OCR results\n",
    "\n",
    "            print(f\"Final OCR + Caption Text: {ocr_text}\")  # Debugging print for combined OCR and caption text\n",
    "\n",
    "            # Use OCR to extract value + unit\n",
    "            prediction = extract_value_and_unit(ocr_text, entity_name)\n",
    "            \n",
    "            # If no prediction from OCR, attempt using GPT model\n",
    "            if not prediction:\n",
    "                prediction = generate_entity_extraction(ocr_text, entity_name)\n",
    "\n",
    "            # Fallback to extract probable numbers if no entity name match\n",
    "            if not prediction:\n",
    "                fallback_numbers = extract_probable_numbers(ocr_text)\n",
    "            else:\n",
    "                fallback_numbers = ''\n",
    "\n",
    "            writer.writerow({'index': index, 'prediction': prediction, 'fallback_numbers': fallback_numbers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final OCR + Caption Text: = 7 O| i\n",
      "\n",
      " a black and white photo of a snow covered area \n",
      "OCR Text: = 7 O| i\n",
      "\n",
      " a black and white photo of a snow covered area \n",
      "No match found for entity height\n",
      "GPT Output: Extract the height from the following text:\n",
      "\n",
      "= 7 O| i\n",
      "\n",
      " a black and white photo of a snow covered area \n",
      "\n",
      "\n",
      "= 7\n",
      "\n",
      "I will try to correct this text.\n",
      "\n",
      "= 7\n",
      "\n",
      "= 7\n",
      "\n",
      "= 7\n",
      "\n",
      "= 7\n",
      "\n",
      "\n",
      "= 7\n",
      "\n",
      "= 7\n",
      "\n",
      "\n",
      "= 7\n",
      "\n",
      "= 7\n",
      "\n",
      "= 7\n",
      "\n",
      "\n",
      "No match found in GPT for entity height\n",
      "Probable numbers found: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final OCR + Caption Text: Size Width Length\n",
      "One Size 42cm/16.54\" 200cmi78.74\"\n",
      " a collage of photos of a person with a camera \n",
      "OCR Text: Size Width Length\n",
      "One Size 42cm/16.54\" 200cmi78.74\"\n",
      " a collage of photos of a person with a camera \n",
      "No match found for entity width\n",
      "GPT Output: Extract the width from the following text:\n",
      "\n",
      "Size Width Length\n",
      "One Size 42cm/16.54\" 200cmi78.74\"\n",
      " a collage of photos of a person with a camera \n",
      "\n",
      "\n",
      "The size of the table is 16.54\" and the height is 23\"\n",
      "\n",
      "The table is 4.16\" and the width is 9\"\n",
      "\n",
      "A table with a camera in this format is 16.54\" and the height\n",
      "No match found in GPT for entity width\n",
      "Probable numbers found: 42, 16.54, 200, 78.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final OCR + Caption Text: Size Width Length\n",
      "One Size 42cm/16.54\" 200cmi78.74\"\n",
      " a collage of photos of a person with a camera \n",
      "OCR Text: Size Width Length\n",
      "One Size 42cm/16.54\" 200cmi78.74\"\n",
      " a collage of photos of a person with a camera \n",
      "No match found for entity height\n",
      "GPT Output: Extract the height from the following text:\n",
      "\n",
      "Size Width Length\n",
      "One Size 42cm/16.54\" 200cmi78.74\"\n",
      " a collage of photos of a person with a camera \n",
      "\n",
      "\n",
      "We have a large group of professional photographers who like to post photos and share them with the world. Join our group and get involved in the community. If so, you may enjoy the image. We are here to help out the community.\n",
      "\n",
      "\n",
      "No match found in GPT for entity height\n",
      "Probable numbers found: 42, 16.54, 200, 78.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final OCR + Caption Text: Size Width Length\n",
      "One Size 42cm/16.54\" 200cmi78.74\"\n",
      " a collage of photos of a person with a camera \n",
      "OCR Text: Size Width Length\n",
      "One Size 42cm/16.54\" 200cmi78.74\"\n",
      " a collage of photos of a person with a camera \n",
      "No match found for entity depth\n",
      "GPT Output: Extract the depth from the following text:\n",
      "\n",
      "Size Width Length\n",
      "One Size 42cm/16.54\" 200cmi78.74\"\n",
      " a collage of photos of a person with a camera \n",
      "\n",
      "\n",
      "Download your photo file and extract it to your PC.\n",
      "\n",
      "\n",
      "You can see that the image is not a square. You may want to change the resolution to be around the size of your image file.\n",
      "\n",
      "You can also see that the\n",
      "No match found in GPT for entity depth\n",
      "Probable numbers found: 42, 16.54, 200, 78.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final OCR + Caption Text: Size Width Length\n",
      "One Size 10.50emi/4.13\" 90cm/35.43\"\n",
      " a collage of photos of people standing in front of a wall \n",
      "OCR Text: Size Width Length\n",
      "One Size 10.50emi/4.13\" 90cm/35.43\"\n",
      " a collage of photos of people standing in front of a wall \n",
      "No match found for entity depth\n",
      "GPT Output: Extract the depth from the following text:\n",
      "\n",
      "Size Width Length\n",
      "One Size 10.50emi/4.13\" 90cm/35.43\"\n",
      " a collage of photos of people standing in front of a wall \n",
      "\n",
      "\n",
      "Now it will open up a number of other ways to do things like make the walls wider or taller.\n",
      "\n",
      "So let's say we want to make the walls wider or taller this way:\n",
      "\n",
      "We can do this by using an algorithm\n",
      "No match found in GPT for entity depth\n",
      "Probable numbers found: 10.50, 4.13, 90, 35.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final OCR + Caption Text: Size Width Length\n",
      "One Size 10.50emi/4.13\" 90cm/35.43\"\n",
      " a collage of photos of people standing in front of a wall \n",
      "OCR Text: Size Width Length\n",
      "One Size 10.50emi/4.13\" 90cm/35.43\"\n",
      " a collage of photos of people standing in front of a wall \n",
      "No match found for entity height\n",
      "GPT Output: Extract the height from the following text:\n",
      "\n",
      "Size Width Length\n",
      "One Size 10.50emi/4.13\" 90cm/35.43\"\n",
      " a collage of photos of people standing in front of a wall \n",
      "\n",
      "\n",
      "To remove the picture, simply double-click on the photo and take the photo off your computer.\n",
      "\n",
      "To remove the picture, simply double-click on the photo and take the photo off your computer. To remove the photo, simply double\n",
      "No match found in GPT for entity height\n",
      "Probable numbers found: 10.50, 4.13, 90, 35.43\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "test_file_path = 'dataset/sample_test.csv'\n",
    "output_file_path = 'output_new.csv'\n",
    "generate_predictions(test_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
